{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency with Lemmatization\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wikipedia](https://en.wikipedia.org/wiki/Lemmatisation): Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "In other words, lemmatization merges the inflected forms of a word into its canonical or \"dictionary\" form. We will use three different lemmatizers to explore how lemmatization impacts word frequency in a bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pattern\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a dataset object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm_client import Dataset\n",
    "\n",
    "# The following line returns the default \"shakespeare\" dataset\n",
    "# dset = Dataset('59c090b6-3851-3c65-e016-9181833b4a2c') \n",
    "\n",
    "# Custom datasets\n",
    "# dset = Dataset('0b5d9a99-d008-f971-3a0e-e2ac7e952fbb')\n",
    "# dset = Dataset('86ff2482-071c-3774-ebba-75704205e8de')\n",
    "dset = Dataset('3fe1090f-5769-ec6a-89b2-25132cbbf576')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the text of the query that built this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"lynching, communism\" from JSTOR from 1900 - 2020'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.query_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find total number of documents in the dataset using the `len()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7629"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q=lynching%2C%20communism&fq=yearPublished%3A%5B1900%20TO%202020%5D&fq=provider%3A(%22jstor%22%20OR%20%22portico%22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'q=lynching, communism&fq=yearPublished:[1900 TO 2020]&fq=provider:(\"jstor\" OR \"portico\")'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dset.query())\n",
    "import urllib\n",
    "\n",
    "urllib.parse.unquote(dset.query())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block takes prints a set of words and their corresponding lemma. You can modify the word list to expermiment with NLTK's lemmatization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Lemma (NLTK)\n",
      "----                 -----\n",
      "be                   be\n",
      "am                   am\n",
      "are                  are\n",
      "is                   is\n",
      "being                being\n",
      "was                  wa\n",
      "were                 were\n",
      "been                 been\n",
      "am not               am not\n",
      "aren't               aren't\n",
      "isn't                isn't\n",
      "wasn't               wasn't\n",
      "weren't              weren't\n",
      "play                 play\n",
      "plays                play\n",
      "playing              playing\n",
      "played               played\n",
      "goose                goose\n",
      "geese                goose\n",
      "good                 good\n",
      "better               better\n",
      "best                 best\n",
      "nice                 nice\n",
      "nicely               nicely\n",
      "fly                  fly\n",
      "flown                flown\n",
      "flew                 flew\n",
      "foot                 foot\n",
      "feet                 foot\n",
      "drink                drink\n",
      "drank                drank\n",
      "drunk                drunk\n",
      "drunks               drunk\n",
      "swim                 swim\n",
      "swims                swim\n",
      "swam                 swam\n"
     ]
    }
   ],
   "source": [
    "words = ['be', 'am', 'are', 'is', 'being', 'was', 'were', 'been', 'am not', \"aren't\", \"isn't\", \"wasn't\", \"weren't\"]\n",
    "words += ['play', 'plays', 'playing', 'played', 'goose', 'geese', 'good','better', 'best', 'nice','nicely']\n",
    "words += ['fly', 'flown', 'flew']\n",
    "words += ['foot', 'feet', 'drink', 'drank', 'drunk', 'drunks', 'swim', 'swims', 'swam']\n",
    "\n",
    "print(\"Word\".ljust(20), \"Lemma (NLTK)\")\n",
    "print(\"----\".ljust(20), \"-----\")\n",
    "for word in words:\n",
    "    print(word.ljust(20), WordNetLemmatizer().lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can see from the results above, by default, NLTK's lemmatizer is limited. In our word list, it only lemmatized nouns. Let's lemmatize the same word list above but this time taking into consideration each word's part of speech (POS). There are several Python libraries that accomplish this task (see [here](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#stanfordcorenlplemmatization) for a discussion of several). We wil use the [spaCy](https://spacy.io/) libary, which you installed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Lemma (spaCy)\n",
      "----                 -----------\n",
      "be                   be\n",
      "am                   be\n",
      "are                  be\n",
      "is                   be\n",
      "being                be\n",
      "was                  be\n",
      "were                 be\n",
      "been                 be\n",
      "am not               be\n",
      "aren't               be\n",
      "isn't                be\n",
      "wasn't               be\n",
      "weren't              be\n",
      "play                 play\n",
      "plays                play\n",
      "playing              play\n",
      "played               play\n",
      "goose                goose\n",
      "geese                geese\n",
      "good                 good\n",
      "better               well\n",
      "best                 good\n",
      "nice                 nice\n",
      "nicely               nicely\n",
      "fly                  fly\n",
      "flown                fly\n",
      "flew                 fly\n",
      "foot                 foot\n",
      "feet                 foot\n",
      "drink                drink\n",
      "drank                drank\n",
      "drunk                drunk\n",
      "drunks               drunk\n",
      "swim                 swim\n",
      "swims                swim\n",
      "swam                 swam\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en', disable=['parser','ner'])\n",
    "\n",
    "print(\"Word\".ljust(20), \"Lemma (spaCy)\")\n",
    "print(\"----\".ljust(20), \"-----------\")\n",
    "for word in words:\n",
    "    print(word.ljust(20), nlp(word)[0].lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try one more popular lemmatizer: [Pattern](https://www.clips.uantwerpen.be/pages/pattern). We will lemmatize the same set of words. Pattern is a web mining library for Python. In addition to natural language processing, it has tools for data mining, machine learning, and network analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Lemma (Pattern)\n",
      "----                 ---------------\n",
      "be                   be\n",
      "am                   be\n",
      "are                  be\n",
      "is                   be\n",
      "being                be\n",
      "was                  be\n",
      "were                 be\n",
      "been                 be\n",
      "am not               be\n",
      "aren't               be\n",
      "isn't                be\n",
      "wasn't               be\n",
      "weren't              be\n",
      "play                 play\n",
      "plays                play\n",
      "playing              play\n",
      "played               play\n",
      "goose                goose\n",
      "geese                geese\n",
      "good                 good\n",
      "better               better\n",
      "best                 best\n",
      "nice                 nice\n",
      "nicely               nicely\n",
      "fly                  fly\n",
      "flown                fly\n",
      "flew                 fly\n",
      "foot                 foot\n",
      "feet                 feet\n",
      "drink                drink\n",
      "drank                drink\n",
      "drunk                drink\n",
      "drunks               drunk\n",
      "swim                 swim\n",
      "swims                swim\n",
      "swam                 swim\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import lemma\n",
    "\n",
    "print(\"Word\".ljust(20), \"Lemma (Pattern)\")\n",
    "print(\"----\".ljust(20), \"---------------\")\n",
    "for word in words:\n",
    "    print(word.ljust(20), lemma(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for processing tokens from the extracted features for volumes in the curated dataset. This function:\n",
    "\n",
    "* lowercases all tokens\n",
    "* discards all tokens less than 4 characters\n",
    "* discards non alphabetical tokens - e.g. --9\n",
    "* removes stopwords using NLTK's stopword list\n",
    "* Lemmatizes the token using NLTK's [WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token(token, do_lemmatizer = False):\n",
    "    token = token.lower()\n",
    "    if len(token) < 4:\n",
    "        return\n",
    "    if not(token.isalpha()):\n",
    "        return\n",
    "    if token in stop_words:\n",
    "        return\n",
    "    if do_lemmatizer:\n",
    "        return WordNetLemmatizer().lemmatize(token)\n",
    "    else:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will loop through and count all of the words in the first 25 articles of your dataset. This code block will NOT lemmatize words; the next two blocks will. The objective is to compare word frequencies with and without lemmatizing your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** 25 Most Frequent Words with NO Lemmatization **\n",
      "american             1793\n",
      "jewish                958\n",
      "world                 881\n",
      "year                  736\n",
      "would                 721\n",
      "also                  612\n",
      "america               576\n",
      "book                  563\n",
      "history               487\n",
      "united                474\n",
      "time                  450\n",
      "people                442\n",
      "university            412\n",
      "jews                  412\n",
      "even                  411\n",
      "york                  411\n",
      "black                 407\n",
      "states                397\n",
      "like                  392\n",
      "many                  389\n",
      "press                 389\n",
      "political             385\n",
      "first                 384\n",
      "could                 376\n",
      "life                  370\n"
     ]
    }
   ],
   "source": [
    "aggr_doc = []\n",
    "\n",
    "for n, unigram_count in enumerate(dset.get_features()):\n",
    "    this_doc = []\n",
    "    for token, count in unigram_count.items():\n",
    "        clean_token = process_token(token, False)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    aggr_doc += this_doc\n",
    "    if n >= 24:\n",
    "        break\n",
    "\n",
    "word_freq = Counter(aggr_doc)\n",
    "print(\"** 25 Most Frequent Words with NO Lemmatization **\")\n",
    "for token, count in word_freq.most_common(25):\n",
    "    print(token.ljust(20), str(count).rjust(4, ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the code block above but with NLTK Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** 25 Most Frequent Words with NTLK Lemmatization **\n",
      "american            2130\n",
      "year                 985\n",
      "jewish               958\n",
      "world                894\n",
      "would                721\n",
      "state                703\n",
      "book                 659\n",
      "also                 612\n",
      "america              584\n",
      "time                 547\n",
      "history              515\n",
      "black                491\n",
      "people               484\n",
      "united               474\n",
      "life                 437\n",
      "right                432\n",
      "university           419\n",
      "jew                  412\n",
      "even                 411\n",
      "york                 411\n",
      "like                 394\n",
      "press                393\n",
      "many                 389\n",
      "political            385\n",
      "first                384\n"
     ]
    }
   ],
   "source": [
    "aggr_doc = []\n",
    "\n",
    "for n, unigram_count in enumerate(dset.get_features()):\n",
    "    this_doc = []\n",
    "    for token, count in unigram_count.items():\n",
    "        clean_token = process_token(token, True)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    aggr_doc += this_doc\n",
    "    if n >= 24:\n",
    "        break\n",
    "\n",
    "word_freq = Counter(aggr_doc)\n",
    "print(\"** 25 Most Frequent Words with NTLK Lemmatization **\")\n",
    "nltk_results = []\n",
    "for token, count in word_freq.most_common(25):\n",
    "    mystr = token.ljust(20) + str(count).rjust(4, ' ')\n",
    "    nltk_results.append(mystr)\n",
    "    print(mystr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare NLTK lemmatization with spaCy and Patter Lemmatization. This may take a few minutes to complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to define token processors like the one for NLTK for spaCy and Pattern\n",
    "def process_token_spacy(token):\n",
    "    token = token.lower()\n",
    "    if len(token) < 4:\n",
    "        return\n",
    "    if not(token.isalpha()):\n",
    "        return\n",
    "    if token in stop_words:\n",
    "        return\n",
    "    return nlp(token)[0].lemma_\n",
    "\n",
    "\n",
    "def process_token_pattern(token):\n",
    "    token = token.lower()\n",
    "    if len(token) < 4:\n",
    "        return\n",
    "    if not(token.isalpha()):\n",
    "        return\n",
    "    if token in stop_words:\n",
    "        return\n",
    "    return lemma(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING__m_oo_m__m_oo_m__m_oo_m__m_oo_m__m_oo_m__m_oo_m__m_oo_m__m_oo_m__m_oo_m__m_oo_m__m_oo_m__m_oo_m__m_oo_m__DONE!!!"
     ]
    }
   ],
   "source": [
    "if not 'nlp' in locals():\n",
    "    nlp = spacy.load('en', disable=['parser','ner'])\n",
    "\n",
    "if not 'pattern' in locals():\n",
    "    from pattern.en import lemma\n",
    "\n",
    "aggr_doc_spacy = []\n",
    "aggr_doc_pattern = []\n",
    "print(\"WORKING_\", end = '')\n",
    "for n, unigram_count in enumerate(dset.get_features()):\n",
    "    if n % 2:\n",
    "        print(\"o_m_\", end = '')\n",
    "    else:\n",
    "        print(\"_m_o\", end = '')\n",
    "    this_doc = []\n",
    "    for token, count in unigram_count.items():\n",
    "        clean_token = process_token_spacy(token)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    aggr_doc_spacy += this_doc\n",
    "    for token, count in unigram_count.items():\n",
    "        clean_token = process_token_pattern(token)\n",
    "        if clean_token is None:\n",
    "            continue\n",
    "        this_doc += [clean_token] * count\n",
    "    aggr_doc_pattern += this_doc\n",
    "    if n >= 24:\n",
    "        break\n",
    "\n",
    "word_freq_spacy = Counter(aggr_doc_spacy)\n",
    "spacy_results = []\n",
    "for token, count in word_freq_spacy.most_common(25):\n",
    "    mystr = token.ljust(20) + str(count).rjust(4, ' ')\n",
    "    spacy_results.append(mystr)\n",
    "    \n",
    "word_freq_pattern = Counter(aggr_doc_pattern)\n",
    "pattern_results = []\n",
    "for token, count in word_freq_pattern.most_common(25):\n",
    "    mystr = token.ljust(20) + str(count).rjust(4, ' ')\n",
    "    pattern_results.append(mystr)\n",
    "\n",
    "print(\"o_m__DONE!!!\", end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's print the results below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* 25 Most Frequent Words with Pattern, spaCy, and NLTK Lemmatization *********\n",
      "------------------------       ------------------------       ------------------------      \n",
      "Pattern Results                spaCy Results                 NLTK Results                  \n",
      "------------------------       ------------------------       ------------------------      \n",
      "american            3923   |   american            1793   |   american            2130\n",
      "year                1970   |   year                 985   |   year                 985\n",
      "jewish              1916   |   jewish               958   |   jewish               958\n",
      "world               1788   |   world                894   |   world                894\n",
      "would               1442   |   would                721   |   would                721\n",
      "state               1432   |   state                716   |   state                703\n",
      "book                1318   |   book                 659   |   book                 659\n",
      "make                1230   |   make                 615   |   also                 612\n",
      "also                1224   |   also                 612   |   america              584\n",
      "america             1160   |   america              576   |   time                 547\n",
      "time                1103   |   time                 549   |   history              515\n",
      "history             1030   |   history              515   |   black                491\n",
      "black                983   |   black                492   |   people               484\n",
      "people               970   |   people               485   |   united               474\n",
      "take                 944   |   united               474   |   life                 437\n",
      "become               932   |   take                 472   |   right                432\n",
      "right                864   |   become               466   |   university           419\n",
      "university           838   |   life                 437   |   jew                  412\n",
      "even                 829   |   right                432   |   even                 411\n",
      "york                 822   |   university           419   |   york                 411\n",
      "work                 812   |   jews                 412   |   like                 394\n",
      "come                 808   |   even                 411   |   press                393\n",
      "life                 807   |   york                 411   |   many                 389\n",
      "like                 798   |   work                 406   |   political            385\n"
     ]
    }
   ],
   "source": [
    "nltk_flag = 'nltk_results' in locals() and len(nltk_results) > 24\n",
    "\n",
    "print(\"********* 25 Most Frequent Words with Pattern, spaCy, and NLTK Lemmatization *********\")\n",
    "print(\"------------------------\".ljust(30), \"------------------------\".ljust(30), \"------------------------\".ljust(30))\n",
    "print(\"Pattern Results\".ljust(30), \"spaCy Results\".ljust(29), \"NLTK Results\".ljust(30))\n",
    "print(\"------------------------\".ljust(30), \"------------------------\".ljust(30), \"------------------------\".ljust(30))\n",
    "for i in range(0,24):\n",
    "    if nltk_flag:\n",
    "        print(pattern_results[i] + \"   |   \" + spacy_results[i]  + \"   |   \" + nltk_results[i])\n",
    "    else:\n",
    "        print(pattern_results[i] + \"   |   \" + spacy_results[i]  + \"   |   \" + \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
